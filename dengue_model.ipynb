{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 88\n",
      "       umidmin    umidmed     umidmax    tempmin    tempmed    tempmax  \\\n",
      "0    63.334366  86.488957   99.304751  18.730769  21.795150  26.653846   \n",
      "1    85.166191  86.695892   88.244707  21.882353  22.215686  22.588235   \n",
      "2    94.158321  94.585553   95.012784  20.857143  20.928571  21.000000   \n",
      "45   56.043362  80.964771   98.259046  17.428571  20.378402  24.571429   \n",
      "46   61.510815  84.295943   98.259188  18.571429  21.394048  26.142857   \n",
      "..         ...        ...         ...        ...        ...        ...   \n",
      "525  67.714286  82.209487   92.285714  19.285714  21.403812  25.285714   \n",
      "618  47.571429  79.537838   99.142857  18.285714  22.404165  28.142857   \n",
      "619  46.285714  82.611648  100.000000  16.857143  21.292295  27.857143   \n",
      "620  60.000000  87.771625  100.000000  16.714286  19.665284  24.714286   \n",
      "621  58.714286  84.591394   98.285714  19.000000  21.994909  27.571429   \n",
      "\n",
      "     precipitacao total  casos  \n",
      "0                  65.0     70  \n",
      "1                 121.4     39  \n",
      "2                  56.4     54  \n",
      "45                 14.2     59  \n",
      "46                  1.6     48  \n",
      "..                  ...    ...  \n",
      "525                77.0     39  \n",
      "618                22.4     44  \n",
      "619                88.4     55  \n",
      "620                44.8     75  \n",
      "621                 1.2     73  \n",
      "\n",
      "[88 rows x 8 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    622.000000\n",
       "mean      12.961415\n",
       "std       12.475577\n",
       "min        0.000000\n",
       "25%        4.000000\n",
       "50%        8.000000\n",
       "75%       17.750000\n",
       "max       38.375000\n",
       "Name: casos, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "data = pd.read_csv('barueri_dengue_filtered.csv').drop('data inicial semana', axis=1).drop('month', axis=1)\n",
    "\n",
    "data_filled = data.fillna(data.mean())\n",
    "\n",
    "Q1 = data_filled['casos'].quantile(0.25)\n",
    "Q3 = data_filled['casos'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "data_filled['casos'] = data_filled['casos'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "\n",
    "X, Y = data_filled[['tempmin','tempmax','precipitacao total']], data_filled['casos']\n",
    "Q1 = data['casos'].quantile(0.25)\n",
    "Q3 = data['casos'].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = data[(data['casos'] < lower_bound) | (data['casos'] > upper_bound)]\n",
    "print(f'Number of outliers: {len(outliers)}')\n",
    "print(outliers)\n",
    "data_filled.casos.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(X, Y):    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y, test_size=0.2, random_state=3)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=3)\n",
    "    \n",
    "    x_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(Y_train.values)\n",
    "    x_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.FloatTensor(Y_val.values)\n",
    "    x_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(Y_test.values)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Casos(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Casos, self).__init__()\n",
    "        self.rede = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.rede(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/eliabe/Dev/Python/Data/DS Bootcamp/.conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_21453/526042166.py\", line 2, in <module>\n",
      "    model = Casos(input_dim)\n",
      "  File \"/tmp/ipykernel_21453/2121181292.py\", line 5, in __init__\n",
      "    nn.Linear(input_dim, 64),\n",
      "  File \"/home/eliabe/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 96, in __init__\n",
      "    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
      "/home/eliabe/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:96: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n"
     ]
    }
   ],
   "source": [
    "input_dim = X.shape[1]\n",
    "model = Casos(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, optimizer, loss_function, train_loader, val_loader, num_epochs, device, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output_train = model(x_batch)\n",
    "            loss_train = loss_function(output_train, y_batch.view(-1, 1))\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss_train.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses[epoch] = train_loss\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val_batch, y_val_batch in val_loader:\n",
    "                x_val_batch, y_val_batch = x_val_batch.to(device), y_val_batch.to(device)\n",
    "                output_val = model(x_val_batch)\n",
    "                loss_val = loss_function(output_val, y_val_batch.view(-1, 1))\n",
    "                val_loss += loss_val.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses[epoch] = val_loss\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}. Best validation loss: {best_val_loss:.4f}')\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 488. Best validation loss: 99.8065\n",
      "Epoch 488/30000, Train Loss: 143.0200, Val Loss: 102.4355\n",
      "Epoch 500/30000, Train Loss: 156.1219, Val Loss: 103.6901\n",
      "Early stopping at epoch 665. Best validation loss: 98.5208\n",
      "Epoch 665/30000, Train Loss: 130.6656, Val Loss: 100.1981\n",
      "Epoch 1000/30000, Train Loss: 133.0383, Val Loss: 98.3362\n",
      "Early stopping at epoch 1148. Best validation loss: 97.3762\n",
      "Epoch 1148/30000, Train Loss: 122.2755, Val Loss: 99.1665\n",
      "Early stopping at epoch 1298. Best validation loss: 97.3762\n",
      "Epoch 1298/30000, Train Loss: 132.0149, Val Loss: 102.4469\n",
      "Early stopping at epoch 1448. Best validation loss: 97.3762\n",
      "Epoch 1448/30000, Train Loss: 139.7183, Val Loss: 101.1288\n",
      "Epoch 1500/30000, Train Loss: 138.4804, Val Loss: 101.6641\n",
      "Early stopping at epoch 1598. Best validation loss: 97.3762\n",
      "Epoch 1598/30000, Train Loss: 128.1979, Val Loss: 101.1162\n",
      "Early stopping at epoch 1748. Best validation loss: 97.3762\n",
      "Epoch 1748/30000, Train Loss: 127.2274, Val Loss: 102.0624\n",
      "Early stopping at epoch 1898. Best validation loss: 97.3762\n",
      "Epoch 1898/30000, Train Loss: 123.5980, Val Loss: 100.9453\n",
      "Epoch 2000/30000, Train Loss: 129.3238, Val Loss: 100.9692\n",
      "Early stopping at epoch 2048. Best validation loss: 97.3762\n",
      "Epoch 2048/30000, Train Loss: 131.9120, Val Loss: 100.1291\n",
      "Early stopping at epoch 2198. Best validation loss: 97.3762\n",
      "Epoch 2198/30000, Train Loss: 131.1764, Val Loss: 101.2348\n",
      "Early stopping at epoch 2348. Best validation loss: 97.3762\n",
      "Epoch 2348/30000, Train Loss: 127.8210, Val Loss: 101.0947\n",
      "Early stopping at epoch 2498. Best validation loss: 97.3762\n",
      "Epoch 2498/30000, Train Loss: 121.4139, Val Loss: 100.8166\n",
      "Epoch 2500/30000, Train Loss: 130.0350, Val Loss: 101.3333\n",
      "Early stopping at epoch 2648. Best validation loss: 97.3762\n",
      "Epoch 2648/30000, Train Loss: 125.8870, Val Loss: 100.7260\n",
      "Early stopping at epoch 2798. Best validation loss: 97.3762\n",
      "Epoch 2798/30000, Train Loss: 120.7469, Val Loss: 100.1736\n",
      "Early stopping at epoch 2948. Best validation loss: 97.3762\n",
      "Epoch 2948/30000, Train Loss: 122.6653, Val Loss: 101.4410\n",
      "Epoch 3000/30000, Train Loss: 125.9535, Val Loss: 102.5056\n",
      "Early stopping at epoch 3098. Best validation loss: 97.3762\n",
      "Epoch 3098/30000, Train Loss: 125.8424, Val Loss: 100.2230\n",
      "Early stopping at epoch 3248. Best validation loss: 97.3762\n",
      "Epoch 3248/30000, Train Loss: 130.6072, Val Loss: 100.6602\n",
      "Early stopping at epoch 3398. Best validation loss: 97.3762\n",
      "Epoch 3398/30000, Train Loss: 121.0670, Val Loss: 101.2893\n",
      "Epoch 3500/30000, Train Loss: 131.2589, Val Loss: 100.5702\n",
      "Early stopping at epoch 3548. Best validation loss: 97.3762\n",
      "Epoch 3548/30000, Train Loss: 118.9229, Val Loss: 99.6858\n",
      "Early stopping at epoch 3698. Best validation loss: 97.3762\n",
      "Epoch 3698/30000, Train Loss: 120.4208, Val Loss: 100.3759\n",
      "Early stopping at epoch 3848. Best validation loss: 97.3762\n",
      "Epoch 3848/30000, Train Loss: 135.3268, Val Loss: 100.6862\n",
      "Early stopping at epoch 3998. Best validation loss: 97.3762\n",
      "Epoch 3998/30000, Train Loss: 132.3159, Val Loss: 99.9506\n",
      "Epoch 4000/30000, Train Loss: 122.4255, Val Loss: 100.5164\n",
      "Early stopping at epoch 4148. Best validation loss: 97.3762\n",
      "Epoch 4148/30000, Train Loss: 134.5669, Val Loss: 100.3158\n",
      "Early stopping at epoch 4298. Best validation loss: 97.3762\n",
      "Epoch 4298/30000, Train Loss: 117.2090, Val Loss: 100.6271\n",
      "Early stopping at epoch 4448. Best validation loss: 97.3762\n",
      "Epoch 4448/30000, Train Loss: 123.5169, Val Loss: 100.0647\n",
      "Epoch 4500/30000, Train Loss: 117.9694, Val Loss: 100.8926\n",
      "Early stopping at epoch 4598. Best validation loss: 97.3762\n",
      "Epoch 4598/30000, Train Loss: 124.6856, Val Loss: 100.4807\n",
      "Early stopping at epoch 4748. Best validation loss: 97.3762\n",
      "Epoch 4748/30000, Train Loss: 129.9106, Val Loss: 101.7157\n",
      "Early stopping at epoch 4898. Best validation loss: 97.3762\n",
      "Epoch 4898/30000, Train Loss: 119.4934, Val Loss: 101.2815\n",
      "Epoch 5000/30000, Train Loss: 127.6310, Val Loss: 100.7881\n",
      "Early stopping at epoch 5048. Best validation loss: 97.3762\n",
      "Epoch 5048/30000, Train Loss: 125.7048, Val Loss: 101.9607\n",
      "Early stopping at epoch 5198. Best validation loss: 97.3762\n",
      "Epoch 5198/30000, Train Loss: 122.8588, Val Loss: 103.3892\n",
      "Early stopping at epoch 5348. Best validation loss: 97.3762\n",
      "Epoch 5348/30000, Train Loss: 125.2973, Val Loss: 101.5440\n",
      "Early stopping at epoch 5498. Best validation loss: 97.3762\n",
      "Epoch 5498/30000, Train Loss: 121.4514, Val Loss: 101.0541\n",
      "Epoch 5500/30000, Train Loss: 121.0379, Val Loss: 100.3832\n",
      "Early stopping at epoch 5648. Best validation loss: 97.3762\n",
      "Epoch 5648/30000, Train Loss: 118.1899, Val Loss: 101.0372\n",
      "Early stopping at epoch 5798. Best validation loss: 97.3762\n",
      "Epoch 5798/30000, Train Loss: 122.5644, Val Loss: 101.3743\n",
      "Early stopping at epoch 5948. Best validation loss: 97.3762\n",
      "Epoch 5948/30000, Train Loss: 127.8625, Val Loss: 103.2110\n",
      "Epoch 6000/30000, Train Loss: 123.3216, Val Loss: 102.1437\n",
      "Early stopping at epoch 6098. Best validation loss: 97.3762\n",
      "Epoch 6098/30000, Train Loss: 124.6610, Val Loss: 100.0659\n",
      "Early stopping at epoch 6248. Best validation loss: 97.3762\n",
      "Epoch 6248/30000, Train Loss: 127.2486, Val Loss: 100.1301\n",
      "Early stopping at epoch 6398. Best validation loss: 97.3762\n",
      "Epoch 6398/30000, Train Loss: 117.1278, Val Loss: 101.0895\n",
      "Epoch 6500/30000, Train Loss: 113.7940, Val Loss: 101.7742\n",
      "Early stopping at epoch 6548. Best validation loss: 97.3762\n",
      "Epoch 6548/30000, Train Loss: 117.9513, Val Loss: 102.4303\n",
      "Early stopping at epoch 6698. Best validation loss: 97.3762\n",
      "Epoch 6698/30000, Train Loss: 130.1247, Val Loss: 102.3176\n",
      "Early stopping at epoch 6848. Best validation loss: 97.3762\n",
      "Epoch 6848/30000, Train Loss: 120.4465, Val Loss: 101.4180\n",
      "Early stopping at epoch 6998. Best validation loss: 97.3762\n",
      "Epoch 6998/30000, Train Loss: 116.4587, Val Loss: 103.1594\n",
      "Epoch 7000/30000, Train Loss: 123.7698, Val Loss: 102.0124\n",
      "Early stopping at epoch 7148. Best validation loss: 97.3762\n",
      "Epoch 7148/30000, Train Loss: 120.6798, Val Loss: 101.1381\n",
      "Early stopping at epoch 7298. Best validation loss: 97.3762\n",
      "Epoch 7298/30000, Train Loss: 121.0151, Val Loss: 102.0594\n",
      "Early stopping at epoch 7448. Best validation loss: 97.3762\n",
      "Epoch 7448/30000, Train Loss: 130.4844, Val Loss: 103.8961\n",
      "Epoch 7500/30000, Train Loss: 128.8523, Val Loss: 104.1554\n",
      "Early stopping at epoch 7598. Best validation loss: 97.3762\n",
      "Epoch 7598/30000, Train Loss: 123.0997, Val Loss: 103.4779\n",
      "Early stopping at epoch 7748. Best validation loss: 97.3762\n",
      "Epoch 7748/30000, Train Loss: 118.4724, Val Loss: 103.9817\n",
      "Early stopping at epoch 7898. Best validation loss: 97.3762\n",
      "Epoch 7898/30000, Train Loss: 123.9365, Val Loss: 101.2004\n",
      "Epoch 8000/30000, Train Loss: 122.2605, Val Loss: 102.2538\n",
      "Early stopping at epoch 8048. Best validation loss: 97.3762\n",
      "Epoch 8048/30000, Train Loss: 122.2353, Val Loss: 103.4469\n",
      "Early stopping at epoch 8198. Best validation loss: 97.3762\n",
      "Epoch 8198/30000, Train Loss: 120.5034, Val Loss: 101.3394\n",
      "Early stopping at epoch 8348. Best validation loss: 97.3762\n",
      "Epoch 8348/30000, Train Loss: 130.0313, Val Loss: 102.3771\n",
      "Early stopping at epoch 8498. Best validation loss: 97.3762\n",
      "Epoch 8498/30000, Train Loss: 121.0141, Val Loss: 102.4700\n",
      "Epoch 8500/30000, Train Loss: 125.1942, Val Loss: 103.0874\n",
      "Early stopping at epoch 8648. Best validation loss: 97.3762\n",
      "Epoch 8648/30000, Train Loss: 121.8699, Val Loss: 102.5308\n",
      "Early stopping at epoch 8798. Best validation loss: 97.3762\n",
      "Epoch 8798/30000, Train Loss: 124.0102, Val Loss: 102.8246\n",
      "Early stopping at epoch 8948. Best validation loss: 97.3762\n",
      "Epoch 8948/30000, Train Loss: 120.2163, Val Loss: 102.6743\n",
      "Epoch 9000/30000, Train Loss: 118.9507, Val Loss: 103.1093\n",
      "Early stopping at epoch 9098. Best validation loss: 97.3762\n",
      "Epoch 9098/30000, Train Loss: 121.7075, Val Loss: 102.4975\n",
      "Early stopping at epoch 9248. Best validation loss: 97.3762\n",
      "Epoch 9248/30000, Train Loss: 121.8040, Val Loss: 103.1185\n",
      "Early stopping at epoch 9398. Best validation loss: 97.3762\n",
      "Epoch 9398/30000, Train Loss: 120.5428, Val Loss: 101.7549\n",
      "Epoch 9500/30000, Train Loss: 123.2338, Val Loss: 103.3673\n",
      "Early stopping at epoch 9548. Best validation loss: 97.3762\n",
      "Epoch 9548/30000, Train Loss: 119.6467, Val Loss: 102.9209\n",
      "Early stopping at epoch 9698. Best validation loss: 97.3762\n",
      "Epoch 9698/30000, Train Loss: 119.7026, Val Loss: 104.1641\n",
      "Early stopping at epoch 9848. Best validation loss: 97.3762\n",
      "Epoch 9848/30000, Train Loss: 127.9349, Val Loss: 103.9956\n",
      "Early stopping at epoch 9998. Best validation loss: 97.3762\n",
      "Epoch 9998/30000, Train Loss: 119.2340, Val Loss: 103.4874\n",
      "Epoch 10000/30000, Train Loss: 116.6964, Val Loss: 103.3191\n",
      "Early stopping at epoch 10148. Best validation loss: 97.3762\n",
      "Epoch 10148/30000, Train Loss: 125.0214, Val Loss: 101.2984\n",
      "Early stopping at epoch 10298. Best validation loss: 97.3762\n",
      "Epoch 10298/30000, Train Loss: 121.4270, Val Loss: 101.2797\n",
      "Early stopping at epoch 10448. Best validation loss: 97.3762\n",
      "Epoch 10448/30000, Train Loss: 125.1664, Val Loss: 102.7063\n",
      "Epoch 10500/30000, Train Loss: 117.2302, Val Loss: 102.3203\n",
      "Early stopping at epoch 10598. Best validation loss: 97.3762\n",
      "Epoch 10598/30000, Train Loss: 118.6516, Val Loss: 101.6498\n",
      "Early stopping at epoch 10748. Best validation loss: 97.3762\n",
      "Epoch 10748/30000, Train Loss: 127.2038, Val Loss: 101.9754\n",
      "Early stopping at epoch 10898. Best validation loss: 97.3762\n",
      "Epoch 10898/30000, Train Loss: 122.8988, Val Loss: 104.2880\n",
      "Epoch 11000/30000, Train Loss: 126.4799, Val Loss: 101.2756\n",
      "Early stopping at epoch 11048. Best validation loss: 97.3762\n",
      "Epoch 11048/30000, Train Loss: 132.7118, Val Loss: 103.3369\n",
      "Early stopping at epoch 11198. Best validation loss: 97.3762\n",
      "Epoch 11198/30000, Train Loss: 121.0531, Val Loss: 103.0828\n",
      "Early stopping at epoch 11348. Best validation loss: 97.3762\n",
      "Epoch 11348/30000, Train Loss: 121.4592, Val Loss: 103.4096\n",
      "Early stopping at epoch 11498. Best validation loss: 97.3762\n",
      "Epoch 11498/30000, Train Loss: 126.2065, Val Loss: 102.5000\n",
      "Epoch 11500/30000, Train Loss: 122.7294, Val Loss: 103.3278\n",
      "Early stopping at epoch 11648. Best validation loss: 97.3762\n",
      "Epoch 11648/30000, Train Loss: 118.1028, Val Loss: 103.0136\n",
      "Early stopping at epoch 11798. Best validation loss: 97.3762\n",
      "Epoch 11798/30000, Train Loss: 122.0069, Val Loss: 101.6213\n",
      "Early stopping at epoch 11948. Best validation loss: 97.3762\n",
      "Epoch 11948/30000, Train Loss: 124.8673, Val Loss: 103.8652\n",
      "Epoch 12000/30000, Train Loss: 117.4135, Val Loss: 101.2477\n",
      "Early stopping at epoch 12098. Best validation loss: 97.3762\n",
      "Epoch 12098/30000, Train Loss: 126.9609, Val Loss: 103.1058\n",
      "Early stopping at epoch 12248. Best validation loss: 97.3762\n",
      "Epoch 12248/30000, Train Loss: 116.0369, Val Loss: 102.3611\n",
      "Early stopping at epoch 12398. Best validation loss: 97.3762\n",
      "Epoch 12398/30000, Train Loss: 122.0165, Val Loss: 104.4340\n",
      "Epoch 12500/30000, Train Loss: 125.3002, Val Loss: 101.4008\n",
      "Early stopping at epoch 12548. Best validation loss: 97.3762\n",
      "Epoch 12548/30000, Train Loss: 115.4238, Val Loss: 103.6050\n",
      "Early stopping at epoch 12698. Best validation loss: 97.3762\n",
      "Epoch 12698/30000, Train Loss: 120.7455, Val Loss: 103.4962\n",
      "Early stopping at epoch 12848. Best validation loss: 97.3762\n",
      "Epoch 12848/30000, Train Loss: 119.7038, Val Loss: 102.0792\n",
      "Early stopping at epoch 12998. Best validation loss: 97.3762\n",
      "Epoch 12998/30000, Train Loss: 121.6352, Val Loss: 103.9259\n",
      "Epoch 13000/30000, Train Loss: 121.8002, Val Loss: 103.1275\n",
      "Early stopping at epoch 13148. Best validation loss: 97.3762\n",
      "Epoch 13148/30000, Train Loss: 122.2247, Val Loss: 103.5661\n",
      "Early stopping at epoch 13298. Best validation loss: 97.3762\n",
      "Epoch 13298/30000, Train Loss: 120.2109, Val Loss: 102.3691\n",
      "Early stopping at epoch 13448. Best validation loss: 97.3762\n",
      "Epoch 13448/30000, Train Loss: 121.3953, Val Loss: 102.0900\n",
      "Epoch 13500/30000, Train Loss: 124.7137, Val Loss: 102.5844\n",
      "Early stopping at epoch 13598. Best validation loss: 97.3762\n",
      "Epoch 13598/30000, Train Loss: 118.9898, Val Loss: 102.9544\n",
      "Early stopping at epoch 13748. Best validation loss: 97.3762\n",
      "Epoch 13748/30000, Train Loss: 117.3240, Val Loss: 102.5541\n",
      "Early stopping at epoch 13898. Best validation loss: 97.3762\n",
      "Epoch 13898/30000, Train Loss: 114.9937, Val Loss: 103.1350\n",
      "Epoch 14000/30000, Train Loss: 127.5111, Val Loss: 103.7788\n",
      "Early stopping at epoch 14048. Best validation loss: 97.3762\n",
      "Epoch 14048/30000, Train Loss: 119.5327, Val Loss: 104.0101\n",
      "Early stopping at epoch 14198. Best validation loss: 97.3762\n",
      "Epoch 14198/30000, Train Loss: 120.8934, Val Loss: 103.2043\n",
      "Early stopping at epoch 14348. Best validation loss: 97.3762\n",
      "Epoch 14348/30000, Train Loss: 121.8130, Val Loss: 102.2621\n",
      "Early stopping at epoch 14498. Best validation loss: 97.3762\n",
      "Epoch 14498/30000, Train Loss: 123.8310, Val Loss: 103.2961\n",
      "Epoch 14500/30000, Train Loss: 121.7056, Val Loss: 102.8975\n",
      "Early stopping at epoch 14648. Best validation loss: 97.3762\n",
      "Epoch 14648/30000, Train Loss: 114.2735, Val Loss: 101.9789\n",
      "Early stopping at epoch 14798. Best validation loss: 97.3762\n",
      "Epoch 14798/30000, Train Loss: 122.0045, Val Loss: 101.9718\n",
      "Early stopping at epoch 14948. Best validation loss: 97.3762\n",
      "Epoch 14948/30000, Train Loss: 123.6788, Val Loss: 104.3032\n",
      "Epoch 15000/30000, Train Loss: 120.1783, Val Loss: 104.1461\n",
      "Early stopping at epoch 15098. Best validation loss: 97.3762\n",
      "Epoch 15098/30000, Train Loss: 117.3404, Val Loss: 103.1571\n",
      "Early stopping at epoch 15248. Best validation loss: 97.3762\n",
      "Epoch 15248/30000, Train Loss: 122.3884, Val Loss: 104.6914\n",
      "Early stopping at epoch 15398. Best validation loss: 97.3762\n",
      "Epoch 15398/30000, Train Loss: 121.7929, Val Loss: 103.0452\n",
      "Epoch 15500/30000, Train Loss: 129.2904, Val Loss: 102.5114\n",
      "Early stopping at epoch 15548. Best validation loss: 97.3762\n",
      "Epoch 15548/30000, Train Loss: 111.9881, Val Loss: 101.9833\n",
      "Early stopping at epoch 15698. Best validation loss: 97.3762\n",
      "Epoch 15698/30000, Train Loss: 123.3109, Val Loss: 102.1098\n",
      "Early stopping at epoch 15848. Best validation loss: 97.3762\n",
      "Epoch 15848/30000, Train Loss: 123.1665, Val Loss: 105.4502\n",
      "Early stopping at epoch 15998. Best validation loss: 97.3762\n",
      "Epoch 15998/30000, Train Loss: 120.0937, Val Loss: 103.9739\n",
      "Epoch 16000/30000, Train Loss: 124.3102, Val Loss: 103.8219\n",
      "Early stopping at epoch 16148. Best validation loss: 97.3762\n",
      "Epoch 16148/30000, Train Loss: 123.0102, Val Loss: 104.7861\n",
      "Early stopping at epoch 16298. Best validation loss: 97.3762\n",
      "Epoch 16298/30000, Train Loss: 119.1562, Val Loss: 101.5822\n",
      "Early stopping at epoch 16448. Best validation loss: 97.3762\n",
      "Epoch 16448/30000, Train Loss: 123.7331, Val Loss: 105.0679\n",
      "Epoch 16500/30000, Train Loss: 119.6579, Val Loss: 101.3036\n",
      "Early stopping at epoch 16598. Best validation loss: 97.3762\n",
      "Epoch 16598/30000, Train Loss: 126.4262, Val Loss: 105.0783\n",
      "Early stopping at epoch 16748. Best validation loss: 97.3762\n",
      "Epoch 16748/30000, Train Loss: 130.3212, Val Loss: 102.7639\n",
      "Early stopping at epoch 16898. Best validation loss: 97.3762\n",
      "Epoch 16898/30000, Train Loss: 119.2923, Val Loss: 102.0083\n",
      "Epoch 17000/30000, Train Loss: 127.8812, Val Loss: 103.6854\n",
      "Early stopping at epoch 17048. Best validation loss: 97.3762\n",
      "Epoch 17048/30000, Train Loss: 122.7586, Val Loss: 103.4448\n",
      "Early stopping at epoch 17198. Best validation loss: 97.3762\n",
      "Epoch 17198/30000, Train Loss: 126.3049, Val Loss: 102.0526\n",
      "Early stopping at epoch 17348. Best validation loss: 97.3762\n",
      "Epoch 17348/30000, Train Loss: 118.7248, Val Loss: 102.7474\n",
      "Early stopping at epoch 17498. Best validation loss: 97.3762\n",
      "Epoch 17498/30000, Train Loss: 123.7035, Val Loss: 102.3976\n",
      "Epoch 17500/30000, Train Loss: 118.5715, Val Loss: 102.2943\n",
      "Early stopping at epoch 17648. Best validation loss: 97.3762\n",
      "Epoch 17648/30000, Train Loss: 128.8453, Val Loss: 103.0500\n",
      "Early stopping at epoch 17798. Best validation loss: 97.3762\n",
      "Epoch 17798/30000, Train Loss: 122.4023, Val Loss: 104.1202\n",
      "Early stopping at epoch 17948. Best validation loss: 97.3762\n",
      "Epoch 17948/30000, Train Loss: 116.4462, Val Loss: 101.7332\n",
      "Epoch 18000/30000, Train Loss: 117.3256, Val Loss: 102.7996\n",
      "Early stopping at epoch 18098. Best validation loss: 97.3762\n",
      "Epoch 18098/30000, Train Loss: 126.0819, Val Loss: 102.6682\n",
      "Early stopping at epoch 18248. Best validation loss: 97.3762\n",
      "Epoch 18248/30000, Train Loss: 120.5006, Val Loss: 102.0944\n",
      "Early stopping at epoch 18398. Best validation loss: 97.3762\n",
      "Epoch 18398/30000, Train Loss: 114.6854, Val Loss: 103.8878\n",
      "Epoch 18500/30000, Train Loss: 125.5572, Val Loss: 103.8273\n",
      "Early stopping at epoch 18548. Best validation loss: 97.3762\n",
      "Epoch 18548/30000, Train Loss: 126.8771, Val Loss: 102.5519\n",
      "Early stopping at epoch 18698. Best validation loss: 97.3762\n",
      "Epoch 18698/30000, Train Loss: 115.9004, Val Loss: 102.3082\n",
      "Early stopping at epoch 18848. Best validation loss: 97.3762\n",
      "Epoch 18848/30000, Train Loss: 119.5034, Val Loss: 103.1808\n",
      "Early stopping at epoch 18998. Best validation loss: 97.3762\n",
      "Epoch 18998/30000, Train Loss: 123.1772, Val Loss: 103.3546\n",
      "Epoch 19000/30000, Train Loss: 130.4898, Val Loss: 103.8920\n",
      "Early stopping at epoch 19148. Best validation loss: 97.3762\n",
      "Epoch 19148/30000, Train Loss: 126.1872, Val Loss: 102.3393\n",
      "Early stopping at epoch 19298. Best validation loss: 97.3762\n",
      "Epoch 19298/30000, Train Loss: 119.1876, Val Loss: 102.4832\n",
      "Early stopping at epoch 19448. Best validation loss: 97.3762\n",
      "Epoch 19448/30000, Train Loss: 129.9801, Val Loss: 102.0930\n",
      "Epoch 19500/30000, Train Loss: 123.6548, Val Loss: 104.0043\n",
      "Early stopping at epoch 19598. Best validation loss: 97.3762\n",
      "Epoch 19598/30000, Train Loss: 123.1907, Val Loss: 103.5437\n",
      "Early stopping at epoch 19748. Best validation loss: 97.3762\n",
      "Epoch 19748/30000, Train Loss: 124.6172, Val Loss: 102.9297\n",
      "Early stopping at epoch 19898. Best validation loss: 97.3762\n",
      "Epoch 19898/30000, Train Loss: 119.0493, Val Loss: 103.4961\n",
      "Epoch 20000/30000, Train Loss: 107.1736, Val Loss: 102.9314\n",
      "Early stopping at epoch 20048. Best validation loss: 97.3762\n",
      "Epoch 20048/30000, Train Loss: 124.0300, Val Loss: 103.0384\n",
      "Early stopping at epoch 20198. Best validation loss: 97.3762\n",
      "Epoch 20198/30000, Train Loss: 118.8979, Val Loss: 104.3858\n",
      "Early stopping at epoch 20348. Best validation loss: 97.3762\n",
      "Epoch 20348/30000, Train Loss: 134.7170, Val Loss: 103.2624\n",
      "Early stopping at epoch 20498. Best validation loss: 97.3762\n",
      "Epoch 20498/30000, Train Loss: 128.6650, Val Loss: 102.7542\n",
      "Epoch 20500/30000, Train Loss: 120.8362, Val Loss: 102.7836\n",
      "Early stopping at epoch 20648. Best validation loss: 97.3762\n",
      "Epoch 20648/30000, Train Loss: 125.2766, Val Loss: 104.7091\n",
      "Early stopping at epoch 20798. Best validation loss: 97.3762\n",
      "Epoch 20798/30000, Train Loss: 118.4108, Val Loss: 104.0615\n",
      "Early stopping at epoch 20948. Best validation loss: 97.3762\n",
      "Epoch 20948/30000, Train Loss: 115.8847, Val Loss: 104.0753\n",
      "Epoch 21000/30000, Train Loss: 125.1214, Val Loss: 103.9229\n",
      "Early stopping at epoch 21098. Best validation loss: 97.3762\n",
      "Epoch 21098/30000, Train Loss: 121.6833, Val Loss: 101.6947\n",
      "Early stopping at epoch 21248. Best validation loss: 97.3762\n",
      "Epoch 21248/30000, Train Loss: 120.3699, Val Loss: 103.1043\n",
      "Early stopping at epoch 21398. Best validation loss: 97.3762\n",
      "Epoch 21398/30000, Train Loss: 124.2058, Val Loss: 104.1738\n",
      "Epoch 21500/30000, Train Loss: 119.4225, Val Loss: 103.7870\n",
      "Early stopping at epoch 21548. Best validation loss: 97.3762\n",
      "Epoch 21548/30000, Train Loss: 117.3200, Val Loss: 104.1217\n",
      "Early stopping at epoch 21698. Best validation loss: 97.3762\n",
      "Epoch 21698/30000, Train Loss: 115.6630, Val Loss: 103.9405\n",
      "Early stopping at epoch 21848. Best validation loss: 97.3762\n",
      "Epoch 21848/30000, Train Loss: 125.7125, Val Loss: 103.9244\n",
      "Early stopping at epoch 21998. Best validation loss: 97.3762\n",
      "Epoch 21998/30000, Train Loss: 117.8781, Val Loss: 103.7706\n",
      "Epoch 22000/30000, Train Loss: 123.8269, Val Loss: 103.9714\n",
      "Early stopping at epoch 22148. Best validation loss: 97.3762\n",
      "Epoch 22148/30000, Train Loss: 115.8556, Val Loss: 102.8619\n",
      "Early stopping at epoch 22298. Best validation loss: 97.3762\n",
      "Epoch 22298/30000, Train Loss: 117.1251, Val Loss: 103.3952\n",
      "Early stopping at epoch 22448. Best validation loss: 97.3762\n",
      "Epoch 22448/30000, Train Loss: 120.5712, Val Loss: 103.2570\n",
      "Epoch 22500/30000, Train Loss: 123.1020, Val Loss: 104.9063\n",
      "Early stopping at epoch 22598. Best validation loss: 97.3762\n",
      "Epoch 22598/30000, Train Loss: 128.3808, Val Loss: 105.4527\n",
      "Early stopping at epoch 22748. Best validation loss: 97.3762\n",
      "Epoch 22748/30000, Train Loss: 123.9188, Val Loss: 104.9082\n",
      "Early stopping at epoch 22898. Best validation loss: 97.3762\n",
      "Epoch 22898/30000, Train Loss: 126.1444, Val Loss: 105.1460\n",
      "Epoch 23000/30000, Train Loss: 127.1428, Val Loss: 104.4377\n",
      "Early stopping at epoch 23048. Best validation loss: 97.3762\n",
      "Epoch 23048/30000, Train Loss: 120.1244, Val Loss: 104.8700\n",
      "Early stopping at epoch 23198. Best validation loss: 97.3762\n",
      "Epoch 23198/30000, Train Loss: 118.2445, Val Loss: 104.2263\n",
      "Early stopping at epoch 23348. Best validation loss: 97.3762\n",
      "Epoch 23348/30000, Train Loss: 119.2276, Val Loss: 105.8469\n",
      "Early stopping at epoch 23498. Best validation loss: 97.3762\n",
      "Epoch 23498/30000, Train Loss: 122.6106, Val Loss: 103.6017\n",
      "Epoch 23500/30000, Train Loss: 124.3699, Val Loss: 103.9063\n",
      "Early stopping at epoch 23648. Best validation loss: 97.3762\n",
      "Epoch 23648/30000, Train Loss: 124.6631, Val Loss: 104.4764\n",
      "Early stopping at epoch 23798. Best validation loss: 97.3762\n",
      "Epoch 23798/30000, Train Loss: 120.9748, Val Loss: 104.6294\n",
      "Early stopping at epoch 23948. Best validation loss: 97.3762\n",
      "Epoch 23948/30000, Train Loss: 128.4558, Val Loss: 103.6057\n",
      "Epoch 24000/30000, Train Loss: 119.4922, Val Loss: 103.0311\n",
      "Early stopping at epoch 24098. Best validation loss: 97.3762\n",
      "Epoch 24098/30000, Train Loss: 118.2758, Val Loss: 103.9609\n",
      "Early stopping at epoch 24248. Best validation loss: 97.3762\n",
      "Epoch 24248/30000, Train Loss: 118.2214, Val Loss: 104.0861\n",
      "Early stopping at epoch 24398. Best validation loss: 97.3762\n",
      "Epoch 24398/30000, Train Loss: 119.7899, Val Loss: 105.0139\n",
      "Epoch 24500/30000, Train Loss: 116.1800, Val Loss: 102.6022\n",
      "Early stopping at epoch 24548. Best validation loss: 97.3762\n",
      "Epoch 24548/30000, Train Loss: 123.9303, Val Loss: 103.6427\n",
      "Early stopping at epoch 24698. Best validation loss: 97.3762\n",
      "Epoch 24698/30000, Train Loss: 127.0872, Val Loss: 102.9307\n",
      "Early stopping at epoch 24848. Best validation loss: 97.3762\n",
      "Epoch 24848/30000, Train Loss: 132.2306, Val Loss: 103.5210\n",
      "Early stopping at epoch 24998. Best validation loss: 97.3762\n",
      "Epoch 24998/30000, Train Loss: 124.7370, Val Loss: 103.1645\n",
      "Epoch 25000/30000, Train Loss: 119.1739, Val Loss: 102.9681\n",
      "Early stopping at epoch 25148. Best validation loss: 97.3762\n",
      "Epoch 25148/30000, Train Loss: 125.9017, Val Loss: 105.3462\n",
      "Early stopping at epoch 25298. Best validation loss: 97.3762\n",
      "Epoch 25298/30000, Train Loss: 115.8129, Val Loss: 103.5453\n",
      "Early stopping at epoch 25448. Best validation loss: 97.3762\n",
      "Epoch 25448/30000, Train Loss: 122.6130, Val Loss: 105.6768\n",
      "Epoch 25500/30000, Train Loss: 126.2296, Val Loss: 104.0998\n",
      "Early stopping at epoch 25598. Best validation loss: 97.3762\n",
      "Epoch 25598/30000, Train Loss: 123.2154, Val Loss: 104.0049\n",
      "Early stopping at epoch 25748. Best validation loss: 97.3762\n",
      "Epoch 25748/30000, Train Loss: 129.4923, Val Loss: 106.1488\n",
      "Early stopping at epoch 25898. Best validation loss: 97.3762\n",
      "Epoch 25898/30000, Train Loss: 123.7516, Val Loss: 103.6372\n",
      "Epoch 26000/30000, Train Loss: 121.6258, Val Loss: 104.1340\n",
      "Early stopping at epoch 26048. Best validation loss: 97.3762\n",
      "Epoch 26048/30000, Train Loss: 122.8837, Val Loss: 103.7705\n",
      "Early stopping at epoch 26198. Best validation loss: 97.3762\n",
      "Epoch 26198/30000, Train Loss: 126.4386, Val Loss: 103.8173\n",
      "Early stopping at epoch 26348. Best validation loss: 97.3762\n",
      "Epoch 26348/30000, Train Loss: 122.7169, Val Loss: 104.0050\n",
      "Early stopping at epoch 26498. Best validation loss: 97.3762\n",
      "Epoch 26498/30000, Train Loss: 124.1356, Val Loss: 104.2178\n",
      "Epoch 26500/30000, Train Loss: 114.7349, Val Loss: 104.1613\n",
      "Early stopping at epoch 26648. Best validation loss: 97.3762\n",
      "Epoch 26648/30000, Train Loss: 124.3702, Val Loss: 103.9349\n",
      "Early stopping at epoch 26798. Best validation loss: 97.3762\n",
      "Epoch 26798/30000, Train Loss: 117.3223, Val Loss: 103.8369\n",
      "Early stopping at epoch 26948. Best validation loss: 97.3762\n",
      "Epoch 26948/30000, Train Loss: 125.5795, Val Loss: 103.8633\n",
      "Epoch 27000/30000, Train Loss: 129.2427, Val Loss: 103.3647\n",
      "Early stopping at epoch 27098. Best validation loss: 97.3762\n",
      "Epoch 27098/30000, Train Loss: 121.3796, Val Loss: 104.8262\n",
      "Early stopping at epoch 27248. Best validation loss: 97.3762\n",
      "Epoch 27248/30000, Train Loss: 125.6479, Val Loss: 105.5312\n",
      "Early stopping at epoch 27398. Best validation loss: 97.3762\n",
      "Epoch 27398/30000, Train Loss: 124.3946, Val Loss: 105.0305\n",
      "Epoch 27500/30000, Train Loss: 126.8488, Val Loss: 103.6763\n",
      "Early stopping at epoch 27548. Best validation loss: 97.3762\n",
      "Epoch 27548/30000, Train Loss: 120.1707, Val Loss: 104.3695\n",
      "Early stopping at epoch 27698. Best validation loss: 97.3762\n",
      "Epoch 27698/30000, Train Loss: 119.2073, Val Loss: 105.9047\n",
      "Early stopping at epoch 27848. Best validation loss: 97.3762\n",
      "Epoch 27848/30000, Train Loss: 119.5231, Val Loss: 102.6539\n",
      "Early stopping at epoch 27998. Best validation loss: 97.3762\n",
      "Epoch 27998/30000, Train Loss: 121.4000, Val Loss: 105.4730\n",
      "Epoch 28000/30000, Train Loss: 119.5573, Val Loss: 104.2353\n",
      "Early stopping at epoch 28148. Best validation loss: 97.3762\n",
      "Epoch 28148/30000, Train Loss: 126.8809, Val Loss: 104.7785\n",
      "Early stopping at epoch 28298. Best validation loss: 97.3762\n",
      "Epoch 28298/30000, Train Loss: 121.9574, Val Loss: 104.1989\n",
      "Early stopping at epoch 28448. Best validation loss: 97.3762\n",
      "Epoch 28448/30000, Train Loss: 131.3606, Val Loss: 105.5886\n",
      "Epoch 28500/30000, Train Loss: 126.3757, Val Loss: 105.1991\n",
      "Early stopping at epoch 28598. Best validation loss: 97.3762\n",
      "Epoch 28598/30000, Train Loss: 119.0427, Val Loss: 105.0681\n",
      "Early stopping at epoch 28748. Best validation loss: 97.3762\n",
      "Epoch 28748/30000, Train Loss: 123.7988, Val Loss: 106.3173\n",
      "Early stopping at epoch 28898. Best validation loss: 97.3762\n",
      "Epoch 28898/30000, Train Loss: 119.1866, Val Loss: 103.6459\n",
      "Epoch 29000/30000, Train Loss: 123.0078, Val Loss: 104.6601\n",
      "Early stopping at epoch 29048. Best validation loss: 97.3762\n",
      "Epoch 29048/30000, Train Loss: 124.9809, Val Loss: 104.6446\n",
      "Early stopping at epoch 29198. Best validation loss: 97.3762\n",
      "Epoch 29198/30000, Train Loss: 124.8200, Val Loss: 106.1756\n",
      "Early stopping at epoch 29348. Best validation loss: 97.3762\n",
      "Epoch 29348/30000, Train Loss: 127.7726, Val Loss: 106.1166\n",
      "Early stopping at epoch 29498. Best validation loss: 97.3762\n",
      "Epoch 29498/30000, Train Loss: 124.1384, Val Loss: 103.8626\n",
      "Epoch 29500/30000, Train Loss: 125.8697, Val Loss: 104.6581\n",
      "Early stopping at epoch 29648. Best validation loss: 97.3762\n",
      "Epoch 29648/30000, Train Loss: 122.2918, Val Loss: 102.6178\n",
      "Early stopping at epoch 29798. Best validation loss: 97.3762\n",
      "Epoch 29798/30000, Train Loss: 114.3397, Val Loss: 104.5955\n",
      "Early stopping at epoch 29948. Best validation loss: 97.3762\n",
      "Epoch 29948/30000, Train Loss: 116.4872, Val Loss: 104.3679\n",
      "Epoch 30000/30000, Train Loss: 122.0369, Val Loss: 104.5452\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "patience = 150\n",
    "num_epochs = 30000\n",
    "train_losses = np.zeros(num_epochs)\n",
    "val_losses = np.zeros(num_epochs)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = load_and_preprocess_data(X, Y)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_network(model, optimizer, criterion, train_loader, val_loader, num_epochs, device, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE: 155.38978931669442\n"
     ]
    }
   ],
   "source": [
    "baseline_prediction = np.mean(Y)\n",
    "baseline_mse = np.mean((Y - baseline_prediction) ** 2)\n",
    "print(f'Baseline MSE: {baseline_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
