{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 88\n",
      "       umidmin    umidmed     umidmax    tempmin    tempmed    tempmax  \\\n",
      "0    63.334366  86.488957   99.304751  18.730769  21.795150  26.653846   \n",
      "1    85.166191  86.695892   88.244707  21.882353  22.215686  22.588235   \n",
      "2    94.158321  94.585553   95.012784  20.857143  20.928571  21.000000   \n",
      "45   56.043362  80.964771   98.259046  17.428571  20.378402  24.571429   \n",
      "46   61.510815  84.295943   98.259188  18.571429  21.394048  26.142857   \n",
      "..         ...        ...         ...        ...        ...        ...   \n",
      "525  67.714286  82.209487   92.285714  19.285714  21.403812  25.285714   \n",
      "618  47.571429  79.537838   99.142857  18.285714  22.404165  28.142857   \n",
      "619  46.285714  82.611648  100.000000  16.857143  21.292295  27.857143   \n",
      "620  60.000000  87.771625  100.000000  16.714286  19.665284  24.714286   \n",
      "621  58.714286  84.591394   98.285714  19.000000  21.994909  27.571429   \n",
      "\n",
      "     precipitacao total  casos  \n",
      "0                  65.0     70  \n",
      "1                 121.4     39  \n",
      "2                  56.4     54  \n",
      "45                 14.2     59  \n",
      "46                  1.6     48  \n",
      "..                  ...    ...  \n",
      "525                77.0     39  \n",
      "618                22.4     44  \n",
      "619                88.4     55  \n",
      "620                44.8     75  \n",
      "621                 1.2     73  \n",
      "\n",
      "[88 rows x 8 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    622.000000\n",
       "mean      12.961415\n",
       "std       12.475577\n",
       "min        0.000000\n",
       "25%        4.000000\n",
       "50%        8.000000\n",
       "75%       17.750000\n",
       "max       38.375000\n",
       "Name: casos, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "data = pd.read_csv('barueri_dengue_filtered.csv').drop('data inicial semana', axis=1).drop('month', axis=1)\n",
    "\n",
    "data_filled = data.fillna(data.mean())\n",
    "\n",
    "Q1 = data_filled['casos'].quantile(0.25)\n",
    "Q3 = data_filled['casos'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "data_filled['casos'] = data_filled['casos'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "\n",
    "X, Y = data_filled[['tempmin','tempmax','precipitacao total']], data_filled['casos']\n",
    "Q1 = data['casos'].quantile(0.25)\n",
    "Q3 = data['casos'].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = data[(data['casos'] < lower_bound) | (data['casos'] > upper_bound)]\n",
    "print(f'Number of outliers: {len(outliers)}')\n",
    "print(outliers)\n",
    "data_filled.casos.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(X, Y):    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y, test_size=0.3, random_state=3)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=3)\n",
    "    \n",
    "    x_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(Y_train.values)\n",
    "    x_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.FloatTensor(Y_val.values)\n",
    "    x_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(Y_test.values)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Casos(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Casos, self).__init__()\n",
    "        self.rede = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.rede(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "model = Casos(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, optimizer, loss_function, train_loader, val_loader, num_epochs, device, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output_train = model(x_batch)\n",
    "            loss_train = loss_function(output_train, y_batch.view(-1, 1))\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss_train.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses[epoch] = train_loss\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val_batch, y_val_batch in val_loader:\n",
    "                x_val_batch, y_val_batch = x_val_batch.to(device), y_val_batch.to(device)\n",
    "                output_val = model(x_val_batch)\n",
    "                loss_val = loss_function(output_val, y_val_batch.view(-1, 1))\n",
    "                val_loss += loss_val.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses[epoch] = val_loss\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}. Best validation loss: {best_val_loss:.4f}')\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            patience_counter = 0\n",
    "            # break\n",
    "        \n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/30000, Train Loss: 154.7980, Val Loss: 84.7383\n",
      "Early stopping at epoch 558. Best validation loss: 71.0123\n",
      "Epoch 558/30000, Train Loss: 153.6307, Val Loss: 89.9337\n",
      "Early stopping at epoch 753. Best validation loss: 69.3665\n",
      "Epoch 753/30000, Train Loss: 143.2345, Val Loss: 80.2185\n",
      "Epoch 1000/30000, Train Loss: 140.9650, Val Loss: 74.1120\n",
      "Early stopping at epoch 1032. Best validation loss: 69.1245\n",
      "Epoch 1032/30000, Train Loss: 139.7387, Val Loss: 93.4799\n",
      "Early stopping at epoch 1316. Best validation loss: 69.0467\n",
      "Epoch 1316/30000, Train Loss: 144.7559, Val Loss: 87.5140\n",
      "Early stopping at epoch 1466. Best validation loss: 69.0467\n",
      "Epoch 1466/30000, Train Loss: 140.1910, Val Loss: 79.2256\n",
      "Epoch 1500/30000, Train Loss: 131.3136, Val Loss: 82.3679\n",
      "Early stopping at epoch 1616. Best validation loss: 69.0467\n",
      "Epoch 1616/30000, Train Loss: 133.1812, Val Loss: 87.7065\n",
      "Early stopping at epoch 1766. Best validation loss: 69.0467\n",
      "Epoch 1766/30000, Train Loss: 133.0627, Val Loss: 90.8300\n",
      "Early stopping at epoch 1916. Best validation loss: 69.0467\n",
      "Epoch 1916/30000, Train Loss: 130.5878, Val Loss: 100.4761\n",
      "Epoch 2000/30000, Train Loss: 129.4509, Val Loss: 87.5947\n",
      "Early stopping at epoch 2066. Best validation loss: 69.0467\n",
      "Epoch 2066/30000, Train Loss: 127.5862, Val Loss: 101.5623\n",
      "Early stopping at epoch 2216. Best validation loss: 69.0467\n",
      "Epoch 2216/30000, Train Loss: 139.3470, Val Loss: 91.8193\n",
      "Early stopping at epoch 2366. Best validation loss: 69.0467\n",
      "Epoch 2366/30000, Train Loss: 132.5178, Val Loss: 91.0431\n",
      "Epoch 2500/30000, Train Loss: 125.6252, Val Loss: 81.9044\n",
      "Early stopping at epoch 2516. Best validation loss: 69.0467\n",
      "Epoch 2516/30000, Train Loss: 127.5317, Val Loss: 80.6558\n",
      "Early stopping at epoch 2666. Best validation loss: 69.0467\n",
      "Epoch 2666/30000, Train Loss: 134.0987, Val Loss: 92.1764\n",
      "Early stopping at epoch 2816. Best validation loss: 69.0467\n",
      "Epoch 2816/30000, Train Loss: 126.6126, Val Loss: 86.6054\n",
      "Early stopping at epoch 2966. Best validation loss: 69.0467\n",
      "Epoch 2966/30000, Train Loss: 138.9890, Val Loss: 89.3444\n",
      "Epoch 3000/30000, Train Loss: 135.3482, Val Loss: 96.5807\n",
      "Early stopping at epoch 3116. Best validation loss: 69.0467\n",
      "Epoch 3116/30000, Train Loss: 126.4245, Val Loss: 99.3920\n",
      "Early stopping at epoch 3266. Best validation loss: 69.0467\n",
      "Epoch 3266/30000, Train Loss: 127.1234, Val Loss: 85.6003\n",
      "Early stopping at epoch 3416. Best validation loss: 69.0467\n",
      "Epoch 3416/30000, Train Loss: 134.9371, Val Loss: 79.9797\n",
      "Epoch 3500/30000, Train Loss: 125.6953, Val Loss: 79.9294\n",
      "Early stopping at epoch 3566. Best validation loss: 69.0467\n",
      "Epoch 3566/30000, Train Loss: 124.7772, Val Loss: 90.3354\n",
      "Early stopping at epoch 3716. Best validation loss: 69.0467\n",
      "Epoch 3716/30000, Train Loss: 131.3545, Val Loss: 78.9209\n",
      "Early stopping at epoch 3866. Best validation loss: 69.0467\n",
      "Epoch 3866/30000, Train Loss: 130.8057, Val Loss: 87.2955\n",
      "Epoch 4000/30000, Train Loss: 131.1986, Val Loss: 76.2358\n",
      "Early stopping at epoch 4016. Best validation loss: 69.0467\n",
      "Epoch 4016/30000, Train Loss: 127.8879, Val Loss: 96.9566\n",
      "Early stopping at epoch 4166. Best validation loss: 69.0467\n",
      "Epoch 4166/30000, Train Loss: 128.3172, Val Loss: 81.2936\n",
      "Early stopping at epoch 4316. Best validation loss: 69.0467\n",
      "Epoch 4316/30000, Train Loss: 130.9450, Val Loss: 93.6660\n",
      "Early stopping at epoch 4466. Best validation loss: 69.0467\n",
      "Epoch 4466/30000, Train Loss: 137.0289, Val Loss: 92.3365\n",
      "Epoch 4500/30000, Train Loss: 132.7401, Val Loss: 81.0219\n",
      "Early stopping at epoch 4616. Best validation loss: 69.0467\n",
      "Epoch 4616/30000, Train Loss: 129.5042, Val Loss: 91.5020\n",
      "Early stopping at epoch 4766. Best validation loss: 69.0467\n",
      "Epoch 4766/30000, Train Loss: 122.1199, Val Loss: 102.1655\n",
      "Early stopping at epoch 4916. Best validation loss: 69.0467\n",
      "Epoch 4916/30000, Train Loss: 131.4718, Val Loss: 90.6206\n",
      "Epoch 5000/30000, Train Loss: 137.2243, Val Loss: 93.1839\n",
      "Early stopping at epoch 5066. Best validation loss: 69.0467\n",
      "Epoch 5066/30000, Train Loss: 140.5315, Val Loss: 90.5861\n",
      "Early stopping at epoch 5216. Best validation loss: 69.0467\n",
      "Epoch 5216/30000, Train Loss: 133.8651, Val Loss: 106.7344\n",
      "Early stopping at epoch 5366. Best validation loss: 69.0467\n",
      "Epoch 5366/30000, Train Loss: 121.1191, Val Loss: 95.7688\n",
      "Epoch 5500/30000, Train Loss: 124.3357, Val Loss: 104.2427\n",
      "Early stopping at epoch 5516. Best validation loss: 69.0467\n",
      "Epoch 5516/30000, Train Loss: 135.5513, Val Loss: 95.3776\n",
      "Early stopping at epoch 5666. Best validation loss: 69.0467\n",
      "Epoch 5666/30000, Train Loss: 128.3083, Val Loss: 102.5059\n",
      "Early stopping at epoch 5816. Best validation loss: 69.0467\n",
      "Epoch 5816/30000, Train Loss: 120.1686, Val Loss: 94.6548\n",
      "Early stopping at epoch 5966. Best validation loss: 69.0467\n",
      "Epoch 5966/30000, Train Loss: 128.8472, Val Loss: 96.9546\n",
      "Epoch 6000/30000, Train Loss: 126.4829, Val Loss: 95.6744\n",
      "Early stopping at epoch 6116. Best validation loss: 69.0467\n",
      "Epoch 6116/30000, Train Loss: 129.1851, Val Loss: 84.3311\n",
      "Early stopping at epoch 6266. Best validation loss: 69.0467\n",
      "Epoch 6266/30000, Train Loss: 134.0946, Val Loss: 98.9174\n",
      "Early stopping at epoch 6416. Best validation loss: 69.0467\n",
      "Epoch 6416/30000, Train Loss: 135.9862, Val Loss: 90.4205\n",
      "Epoch 6500/30000, Train Loss: 129.1822, Val Loss: 84.8759\n",
      "Early stopping at epoch 6566. Best validation loss: 69.0467\n",
      "Epoch 6566/30000, Train Loss: 132.3111, Val Loss: 84.8706\n",
      "Early stopping at epoch 6716. Best validation loss: 69.0467\n",
      "Epoch 6716/30000, Train Loss: 129.5000, Val Loss: 91.7220\n",
      "Early stopping at epoch 6866. Best validation loss: 69.0467\n",
      "Epoch 6866/30000, Train Loss: 129.4521, Val Loss: 87.3178\n",
      "Epoch 7000/30000, Train Loss: 136.8283, Val Loss: 96.8263\n",
      "Early stopping at epoch 7016. Best validation loss: 69.0467\n",
      "Epoch 7016/30000, Train Loss: 130.6642, Val Loss: 85.4651\n",
      "Early stopping at epoch 7166. Best validation loss: 69.0467\n",
      "Epoch 7166/30000, Train Loss: 126.3131, Val Loss: 90.1952\n",
      "Early stopping at epoch 7316. Best validation loss: 69.0467\n",
      "Epoch 7316/30000, Train Loss: 120.8214, Val Loss: 88.1068\n",
      "Early stopping at epoch 7466. Best validation loss: 69.0467\n",
      "Epoch 7466/30000, Train Loss: 125.1331, Val Loss: 83.5265\n",
      "Epoch 7500/30000, Train Loss: 126.4804, Val Loss: 98.6219\n",
      "Early stopping at epoch 7616. Best validation loss: 69.0467\n",
      "Epoch 7616/30000, Train Loss: 125.2625, Val Loss: 92.6560\n",
      "Early stopping at epoch 7766. Best validation loss: 69.0467\n",
      "Epoch 7766/30000, Train Loss: 128.2048, Val Loss: 90.3701\n",
      "Early stopping at epoch 7916. Best validation loss: 69.0467\n",
      "Epoch 7916/30000, Train Loss: 125.0934, Val Loss: 91.1761\n",
      "Epoch 8000/30000, Train Loss: 133.7127, Val Loss: 92.3176\n",
      "Early stopping at epoch 8066. Best validation loss: 69.0467\n",
      "Epoch 8066/30000, Train Loss: 131.8088, Val Loss: 99.7649\n",
      "Early stopping at epoch 8216. Best validation loss: 69.0467\n",
      "Epoch 8216/30000, Train Loss: 125.8393, Val Loss: 97.5475\n",
      "Early stopping at epoch 8366. Best validation loss: 69.0467\n",
      "Epoch 8366/30000, Train Loss: 126.7937, Val Loss: 98.5423\n",
      "Epoch 8500/30000, Train Loss: 123.8864, Val Loss: 82.2698\n",
      "Early stopping at epoch 8516. Best validation loss: 69.0467\n",
      "Epoch 8516/30000, Train Loss: 126.7164, Val Loss: 88.0169\n",
      "Early stopping at epoch 8666. Best validation loss: 69.0467\n",
      "Epoch 8666/30000, Train Loss: 122.7682, Val Loss: 84.8784\n",
      "Early stopping at epoch 8816. Best validation loss: 69.0467\n",
      "Epoch 8816/30000, Train Loss: 121.5956, Val Loss: 89.8880\n",
      "Early stopping at epoch 8966. Best validation loss: 69.0467\n",
      "Epoch 8966/30000, Train Loss: 123.8815, Val Loss: 92.5485\n",
      "Epoch 9000/30000, Train Loss: 127.3555, Val Loss: 93.5107\n",
      "Early stopping at epoch 9116. Best validation loss: 69.0467\n",
      "Epoch 9116/30000, Train Loss: 130.3407, Val Loss: 90.8799\n",
      "Early stopping at epoch 9266. Best validation loss: 69.0467\n",
      "Epoch 9266/30000, Train Loss: 127.6826, Val Loss: 96.1360\n",
      "Early stopping at epoch 9416. Best validation loss: 69.0467\n",
      "Epoch 9416/30000, Train Loss: 126.4043, Val Loss: 78.2428\n",
      "Epoch 9500/30000, Train Loss: 123.4619, Val Loss: 88.9699\n",
      "Early stopping at epoch 9566. Best validation loss: 69.0467\n",
      "Epoch 9566/30000, Train Loss: 123.0827, Val Loss: 83.9895\n",
      "Early stopping at epoch 9716. Best validation loss: 69.0467\n",
      "Epoch 9716/30000, Train Loss: 128.3888, Val Loss: 92.6009\n",
      "Early stopping at epoch 9866. Best validation loss: 69.0467\n",
      "Epoch 9866/30000, Train Loss: 126.2789, Val Loss: 101.3783\n",
      "Epoch 10000/30000, Train Loss: 128.2903, Val Loss: 76.6541\n",
      "Early stopping at epoch 10016. Best validation loss: 69.0467\n",
      "Epoch 10016/30000, Train Loss: 136.8058, Val Loss: 90.3501\n",
      "Early stopping at epoch 10166. Best validation loss: 69.0467\n",
      "Epoch 10166/30000, Train Loss: 129.9472, Val Loss: 89.2978\n",
      "Early stopping at epoch 10316. Best validation loss: 69.0467\n",
      "Epoch 10316/30000, Train Loss: 134.7464, Val Loss: 83.0556\n",
      "Early stopping at epoch 10466. Best validation loss: 69.0467\n",
      "Epoch 10466/30000, Train Loss: 131.8499, Val Loss: 95.0768\n",
      "Epoch 10500/30000, Train Loss: 122.9671, Val Loss: 84.0774\n",
      "Early stopping at epoch 10616. Best validation loss: 69.0467\n",
      "Epoch 10616/30000, Train Loss: 122.3584, Val Loss: 91.0938\n",
      "Early stopping at epoch 10766. Best validation loss: 69.0467\n",
      "Epoch 10766/30000, Train Loss: 128.5164, Val Loss: 90.0930\n",
      "Early stopping at epoch 10916. Best validation loss: 69.0467\n",
      "Epoch 10916/30000, Train Loss: 133.7526, Val Loss: 89.6090\n",
      "Epoch 11000/30000, Train Loss: 124.4204, Val Loss: 79.0844\n",
      "Early stopping at epoch 11066. Best validation loss: 69.0467\n",
      "Epoch 11066/30000, Train Loss: 125.4974, Val Loss: 95.6615\n",
      "Early stopping at epoch 11216. Best validation loss: 69.0467\n",
      "Epoch 11216/30000, Train Loss: 125.1256, Val Loss: 100.8711\n",
      "Early stopping at epoch 11366. Best validation loss: 69.0467\n",
      "Epoch 11366/30000, Train Loss: 127.8794, Val Loss: 98.1161\n",
      "Epoch 11500/30000, Train Loss: 128.4460, Val Loss: 83.8365\n",
      "Early stopping at epoch 11516. Best validation loss: 69.0467\n",
      "Epoch 11516/30000, Train Loss: 120.0464, Val Loss: 88.9431\n",
      "Early stopping at epoch 11666. Best validation loss: 69.0467\n",
      "Epoch 11666/30000, Train Loss: 128.6984, Val Loss: 86.3372\n",
      "Early stopping at epoch 11816. Best validation loss: 69.0467\n",
      "Epoch 11816/30000, Train Loss: 124.4787, Val Loss: 89.2091\n",
      "Early stopping at epoch 11966. Best validation loss: 69.0467\n",
      "Epoch 11966/30000, Train Loss: 124.8968, Val Loss: 97.2822\n",
      "Epoch 12000/30000, Train Loss: 128.1752, Val Loss: 90.7850\n",
      "Early stopping at epoch 12116. Best validation loss: 69.0467\n",
      "Epoch 12116/30000, Train Loss: 123.4947, Val Loss: 94.4164\n",
      "Early stopping at epoch 12266. Best validation loss: 69.0467\n",
      "Epoch 12266/30000, Train Loss: 132.2287, Val Loss: 78.2868\n",
      "Early stopping at epoch 12416. Best validation loss: 69.0467\n",
      "Epoch 12416/30000, Train Loss: 129.7539, Val Loss: 99.7863\n",
      "Epoch 12500/30000, Train Loss: 131.2911, Val Loss: 97.6177\n",
      "Early stopping at epoch 12566. Best validation loss: 69.0467\n",
      "Epoch 12566/30000, Train Loss: 122.3137, Val Loss: 100.7087\n",
      "Early stopping at epoch 12716. Best validation loss: 69.0467\n",
      "Epoch 12716/30000, Train Loss: 129.1407, Val Loss: 92.3202\n",
      "Early stopping at epoch 12866. Best validation loss: 69.0467\n",
      "Epoch 12866/30000, Train Loss: 132.8238, Val Loss: 79.9878\n",
      "Epoch 13000/30000, Train Loss: 130.4818, Val Loss: 99.7585\n",
      "Early stopping at epoch 13016. Best validation loss: 69.0467\n",
      "Epoch 13016/30000, Train Loss: 125.4643, Val Loss: 102.1049\n",
      "Early stopping at epoch 13166. Best validation loss: 69.0467\n",
      "Epoch 13166/30000, Train Loss: 127.6226, Val Loss: 79.8780\n",
      "Early stopping at epoch 13316. Best validation loss: 69.0467\n",
      "Epoch 13316/30000, Train Loss: 135.1090, Val Loss: 98.0619\n",
      "Early stopping at epoch 13466. Best validation loss: 69.0467\n",
      "Epoch 13466/30000, Train Loss: 132.5106, Val Loss: 93.1891\n",
      "Epoch 13500/30000, Train Loss: 133.6962, Val Loss: 74.4445\n",
      "Early stopping at epoch 13616. Best validation loss: 69.0467\n",
      "Epoch 13616/30000, Train Loss: 131.3251, Val Loss: 84.1582\n",
      "Early stopping at epoch 13766. Best validation loss: 69.0467\n",
      "Epoch 13766/30000, Train Loss: 123.7306, Val Loss: 83.7992\n",
      "Early stopping at epoch 13916. Best validation loss: 69.0467\n",
      "Epoch 13916/30000, Train Loss: 130.8502, Val Loss: 98.6558\n",
      "Epoch 14000/30000, Train Loss: 127.6423, Val Loss: 89.7786\n",
      "Early stopping at epoch 14066. Best validation loss: 69.0467\n",
      "Epoch 14066/30000, Train Loss: 125.9706, Val Loss: 86.2020\n",
      "Early stopping at epoch 14216. Best validation loss: 69.0467\n",
      "Epoch 14216/30000, Train Loss: 126.9165, Val Loss: 98.6623\n",
      "Early stopping at epoch 14366. Best validation loss: 69.0467\n",
      "Epoch 14366/30000, Train Loss: 122.9238, Val Loss: 80.2365\n",
      "Epoch 14500/30000, Train Loss: 125.8488, Val Loss: 97.5808\n",
      "Early stopping at epoch 14516. Best validation loss: 69.0467\n",
      "Epoch 14516/30000, Train Loss: 131.7029, Val Loss: 95.5656\n",
      "Early stopping at epoch 14666. Best validation loss: 69.0467\n",
      "Epoch 14666/30000, Train Loss: 131.9193, Val Loss: 77.5739\n",
      "Early stopping at epoch 14816. Best validation loss: 69.0467\n",
      "Epoch 14816/30000, Train Loss: 120.8296, Val Loss: 91.2965\n",
      "Early stopping at epoch 14966. Best validation loss: 69.0467\n",
      "Epoch 14966/30000, Train Loss: 123.0300, Val Loss: 87.7612\n",
      "Epoch 15000/30000, Train Loss: 125.7655, Val Loss: 91.2428\n",
      "Early stopping at epoch 15116. Best validation loss: 69.0467\n",
      "Epoch 15116/30000, Train Loss: 119.6895, Val Loss: 95.4569\n",
      "Early stopping at epoch 15266. Best validation loss: 69.0467\n",
      "Epoch 15266/30000, Train Loss: 133.1287, Val Loss: 99.4444\n",
      "Early stopping at epoch 15416. Best validation loss: 69.0467\n",
      "Epoch 15416/30000, Train Loss: 138.7530, Val Loss: 100.0556\n",
      "Epoch 15500/30000, Train Loss: 124.1009, Val Loss: 82.3514\n",
      "Early stopping at epoch 15566. Best validation loss: 69.0467\n",
      "Epoch 15566/30000, Train Loss: 123.4586, Val Loss: 87.4523\n",
      "Early stopping at epoch 15716. Best validation loss: 69.0467\n",
      "Epoch 15716/30000, Train Loss: 123.8968, Val Loss: 89.4517\n",
      "Early stopping at epoch 15866. Best validation loss: 69.0467\n",
      "Epoch 15866/30000, Train Loss: 134.0532, Val Loss: 103.4844\n",
      "Epoch 16000/30000, Train Loss: 138.4219, Val Loss: 79.7194\n",
      "Early stopping at epoch 16016. Best validation loss: 69.0467\n",
      "Epoch 16016/30000, Train Loss: 132.5272, Val Loss: 82.2301\n",
      "Early stopping at epoch 16166. Best validation loss: 69.0467\n",
      "Epoch 16166/30000, Train Loss: 127.1161, Val Loss: 78.3581\n",
      "Early stopping at epoch 16316. Best validation loss: 69.0467\n",
      "Epoch 16316/30000, Train Loss: 117.4979, Val Loss: 77.6928\n",
      "Early stopping at epoch 16466. Best validation loss: 69.0467\n",
      "Epoch 16466/30000, Train Loss: 126.0198, Val Loss: 88.1850\n",
      "Epoch 16500/30000, Train Loss: 132.5455, Val Loss: 100.3354\n",
      "Early stopping at epoch 16616. Best validation loss: 69.0467\n",
      "Epoch 16616/30000, Train Loss: 122.8952, Val Loss: 105.0561\n",
      "Early stopping at epoch 16766. Best validation loss: 69.0467\n",
      "Epoch 16766/30000, Train Loss: 124.9969, Val Loss: 99.2105\n",
      "Early stopping at epoch 16916. Best validation loss: 69.0467\n",
      "Epoch 16916/30000, Train Loss: 125.6898, Val Loss: 82.2746\n",
      "Epoch 17000/30000, Train Loss: 132.0761, Val Loss: 87.9809\n",
      "Early stopping at epoch 17066. Best validation loss: 69.0467\n",
      "Epoch 17066/30000, Train Loss: 131.4802, Val Loss: 103.3056\n",
      "Early stopping at epoch 17216. Best validation loss: 69.0467\n",
      "Epoch 17216/30000, Train Loss: 130.4975, Val Loss: 98.7851\n",
      "Early stopping at epoch 17366. Best validation loss: 69.0467\n",
      "Epoch 17366/30000, Train Loss: 131.2483, Val Loss: 96.3894\n",
      "Epoch 17500/30000, Train Loss: 125.4646, Val Loss: 91.7932\n",
      "Early stopping at epoch 17516. Best validation loss: 69.0467\n",
      "Epoch 17516/30000, Train Loss: 121.7393, Val Loss: 95.9312\n",
      "Early stopping at epoch 17666. Best validation loss: 69.0467\n",
      "Epoch 17666/30000, Train Loss: 119.3690, Val Loss: 91.9047\n",
      "Early stopping at epoch 17816. Best validation loss: 69.0467\n",
      "Epoch 17816/30000, Train Loss: 122.1648, Val Loss: 98.8895\n",
      "Early stopping at epoch 17966. Best validation loss: 69.0467\n",
      "Epoch 17966/30000, Train Loss: 121.6983, Val Loss: 91.0509\n",
      "Epoch 18000/30000, Train Loss: 127.5068, Val Loss: 94.7255\n",
      "Early stopping at epoch 18116. Best validation loss: 69.0467\n",
      "Epoch 18116/30000, Train Loss: 120.0010, Val Loss: 82.4317\n",
      "Early stopping at epoch 18266. Best validation loss: 69.0467\n",
      "Epoch 18266/30000, Train Loss: 121.1584, Val Loss: 101.0885\n",
      "Early stopping at epoch 18416. Best validation loss: 69.0467\n",
      "Epoch 18416/30000, Train Loss: 135.1186, Val Loss: 102.1779\n",
      "Epoch 18500/30000, Train Loss: 122.5294, Val Loss: 81.7251\n",
      "Early stopping at epoch 18566. Best validation loss: 69.0467\n",
      "Epoch 18566/30000, Train Loss: 120.9605, Val Loss: 103.3426\n",
      "Early stopping at epoch 18716. Best validation loss: 69.0467\n",
      "Epoch 18716/30000, Train Loss: 130.7534, Val Loss: 82.1605\n",
      "Early stopping at epoch 18866. Best validation loss: 69.0467\n",
      "Epoch 18866/30000, Train Loss: 133.5604, Val Loss: 90.9854\n",
      "Epoch 19000/30000, Train Loss: 119.9269, Val Loss: 89.7153\n",
      "Early stopping at epoch 19016. Best validation loss: 69.0467\n",
      "Epoch 19016/30000, Train Loss: 118.7566, Val Loss: 81.3184\n",
      "Early stopping at epoch 19166. Best validation loss: 69.0467\n",
      "Epoch 19166/30000, Train Loss: 127.2581, Val Loss: 83.5877\n",
      "Early stopping at epoch 19316. Best validation loss: 69.0467\n",
      "Epoch 19316/30000, Train Loss: 127.3717, Val Loss: 86.1302\n",
      "Early stopping at epoch 19466. Best validation loss: 69.0467\n",
      "Epoch 19466/30000, Train Loss: 124.6884, Val Loss: 84.5493\n",
      "Epoch 19500/30000, Train Loss: 123.2341, Val Loss: 101.0670\n",
      "Early stopping at epoch 19616. Best validation loss: 69.0467\n",
      "Epoch 19616/30000, Train Loss: 123.3304, Val Loss: 101.7729\n",
      "Early stopping at epoch 19766. Best validation loss: 69.0467\n",
      "Epoch 19766/30000, Train Loss: 124.3634, Val Loss: 78.1208\n",
      "Early stopping at epoch 19916. Best validation loss: 69.0467\n",
      "Epoch 19916/30000, Train Loss: 126.3822, Val Loss: 84.6096\n",
      "Epoch 20000/30000, Train Loss: 125.2168, Val Loss: 90.2463\n",
      "Early stopping at epoch 20066. Best validation loss: 69.0467\n",
      "Epoch 20066/30000, Train Loss: 121.5726, Val Loss: 81.0338\n",
      "Early stopping at epoch 20216. Best validation loss: 69.0467\n",
      "Epoch 20216/30000, Train Loss: 128.8080, Val Loss: 98.3649\n",
      "Early stopping at epoch 20366. Best validation loss: 69.0467\n",
      "Epoch 20366/30000, Train Loss: 122.1427, Val Loss: 106.6373\n",
      "Epoch 20500/30000, Train Loss: 122.6458, Val Loss: 94.2525\n",
      "Early stopping at epoch 20516. Best validation loss: 69.0467\n",
      "Epoch 20516/30000, Train Loss: 132.7063, Val Loss: 97.4937\n",
      "Early stopping at epoch 20666. Best validation loss: 69.0467\n",
      "Epoch 20666/30000, Train Loss: 128.2636, Val Loss: 99.7015\n",
      "Early stopping at epoch 20816. Best validation loss: 69.0467\n",
      "Epoch 20816/30000, Train Loss: 130.3047, Val Loss: 94.5590\n",
      "Early stopping at epoch 20966. Best validation loss: 69.0467\n",
      "Epoch 20966/30000, Train Loss: 136.8929, Val Loss: 77.4389\n",
      "Epoch 21000/30000, Train Loss: 124.9551, Val Loss: 91.5022\n",
      "Early stopping at epoch 21116. Best validation loss: 69.0467\n",
      "Epoch 21116/30000, Train Loss: 119.9255, Val Loss: 83.5330\n",
      "Early stopping at epoch 21266. Best validation loss: 69.0467\n",
      "Epoch 21266/30000, Train Loss: 127.7717, Val Loss: 87.2347\n",
      "Early stopping at epoch 21416. Best validation loss: 69.0467\n",
      "Epoch 21416/30000, Train Loss: 125.4079, Val Loss: 95.0487\n",
      "Epoch 21500/30000, Train Loss: 125.0740, Val Loss: 95.7834\n",
      "Early stopping at epoch 21566. Best validation loss: 69.0467\n",
      "Epoch 21566/30000, Train Loss: 122.4967, Val Loss: 92.9899\n",
      "Early stopping at epoch 21716. Best validation loss: 69.0467\n",
      "Epoch 21716/30000, Train Loss: 131.2245, Val Loss: 102.3178\n",
      "Early stopping at epoch 21866. Best validation loss: 69.0467\n",
      "Epoch 21866/30000, Train Loss: 117.7079, Val Loss: 98.0068\n",
      "Epoch 22000/30000, Train Loss: 120.6760, Val Loss: 93.3740\n",
      "Early stopping at epoch 22016. Best validation loss: 69.0467\n",
      "Epoch 22016/30000, Train Loss: 137.6899, Val Loss: 96.2710\n",
      "Early stopping at epoch 22166. Best validation loss: 69.0467\n",
      "Epoch 22166/30000, Train Loss: 124.3927, Val Loss: 102.1464\n",
      "Early stopping at epoch 22316. Best validation loss: 69.0467\n",
      "Epoch 22316/30000, Train Loss: 118.5911, Val Loss: 97.6773\n",
      "Early stopping at epoch 22466. Best validation loss: 69.0467\n",
      "Epoch 22466/30000, Train Loss: 134.1577, Val Loss: 91.2905\n",
      "Epoch 22500/30000, Train Loss: 128.1222, Val Loss: 101.1160\n",
      "Early stopping at epoch 22616. Best validation loss: 69.0467\n",
      "Epoch 22616/30000, Train Loss: 131.6307, Val Loss: 87.3477\n",
      "Early stopping at epoch 22766. Best validation loss: 69.0467\n",
      "Epoch 22766/30000, Train Loss: 126.1579, Val Loss: 96.5728\n",
      "Early stopping at epoch 22916. Best validation loss: 69.0467\n",
      "Epoch 22916/30000, Train Loss: 123.5301, Val Loss: 95.8763\n",
      "Epoch 23000/30000, Train Loss: 117.2346, Val Loss: 88.5408\n",
      "Early stopping at epoch 23066. Best validation loss: 69.0467\n",
      "Epoch 23066/30000, Train Loss: 116.6254, Val Loss: 85.3615\n",
      "Early stopping at epoch 23216. Best validation loss: 69.0467\n",
      "Epoch 23216/30000, Train Loss: 123.0242, Val Loss: 85.1691\n",
      "Early stopping at epoch 23366. Best validation loss: 69.0467\n",
      "Epoch 23366/30000, Train Loss: 132.2498, Val Loss: 91.4612\n",
      "Epoch 23500/30000, Train Loss: 126.5799, Val Loss: 83.9390\n",
      "Early stopping at epoch 23516. Best validation loss: 69.0467\n",
      "Epoch 23516/30000, Train Loss: 127.1908, Val Loss: 87.0552\n",
      "Early stopping at epoch 23666. Best validation loss: 69.0467\n",
      "Epoch 23666/30000, Train Loss: 128.0783, Val Loss: 92.9613\n",
      "Early stopping at epoch 23816. Best validation loss: 69.0467\n",
      "Epoch 23816/30000, Train Loss: 123.0114, Val Loss: 82.2815\n",
      "Early stopping at epoch 23966. Best validation loss: 69.0467\n",
      "Epoch 23966/30000, Train Loss: 129.0846, Val Loss: 91.9145\n",
      "Epoch 24000/30000, Train Loss: 122.7138, Val Loss: 87.5221\n",
      "Early stopping at epoch 24116. Best validation loss: 69.0467\n",
      "Epoch 24116/30000, Train Loss: 130.4341, Val Loss: 100.8117\n",
      "Early stopping at epoch 24266. Best validation loss: 69.0467\n",
      "Epoch 24266/30000, Train Loss: 115.2629, Val Loss: 88.7429\n",
      "Early stopping at epoch 24416. Best validation loss: 69.0467\n",
      "Epoch 24416/30000, Train Loss: 131.3587, Val Loss: 80.5520\n",
      "Epoch 24500/30000, Train Loss: 122.0842, Val Loss: 89.9550\n",
      "Early stopping at epoch 24566. Best validation loss: 69.0467\n",
      "Epoch 24566/30000, Train Loss: 124.1220, Val Loss: 84.9208\n",
      "Early stopping at epoch 24716. Best validation loss: 69.0467\n",
      "Epoch 24716/30000, Train Loss: 128.8146, Val Loss: 90.3455\n",
      "Early stopping at epoch 24866. Best validation loss: 69.0467\n",
      "Epoch 24866/30000, Train Loss: 125.9294, Val Loss: 95.2117\n",
      "Epoch 25000/30000, Train Loss: 119.2223, Val Loss: 94.4474\n",
      "Early stopping at epoch 25016. Best validation loss: 69.0467\n",
      "Epoch 25016/30000, Train Loss: 125.0797, Val Loss: 90.1154\n",
      "Early stopping at epoch 25166. Best validation loss: 69.0467\n",
      "Epoch 25166/30000, Train Loss: 122.6292, Val Loss: 79.8424\n",
      "Early stopping at epoch 25316. Best validation loss: 69.0467\n",
      "Epoch 25316/30000, Train Loss: 121.0730, Val Loss: 83.0134\n",
      "Early stopping at epoch 25466. Best validation loss: 69.0467\n",
      "Epoch 25466/30000, Train Loss: 128.3840, Val Loss: 89.5267\n",
      "Epoch 25500/30000, Train Loss: 129.4559, Val Loss: 83.7437\n",
      "Early stopping at epoch 25616. Best validation loss: 69.0467\n",
      "Epoch 25616/30000, Train Loss: 120.6276, Val Loss: 98.5195\n",
      "Early stopping at epoch 25766. Best validation loss: 69.0467\n",
      "Epoch 25766/30000, Train Loss: 120.6587, Val Loss: 87.2816\n",
      "Early stopping at epoch 25916. Best validation loss: 69.0467\n",
      "Epoch 25916/30000, Train Loss: 120.5763, Val Loss: 88.4620\n",
      "Epoch 26000/30000, Train Loss: 133.7591, Val Loss: 87.9383\n",
      "Early stopping at epoch 26066. Best validation loss: 69.0467\n",
      "Epoch 26066/30000, Train Loss: 131.1561, Val Loss: 90.1725\n",
      "Early stopping at epoch 26216. Best validation loss: 69.0467\n",
      "Epoch 26216/30000, Train Loss: 127.8981, Val Loss: 97.7724\n",
      "Early stopping at epoch 26366. Best validation loss: 69.0467\n",
      "Epoch 26366/30000, Train Loss: 128.3692, Val Loss: 89.6137\n",
      "Epoch 26500/30000, Train Loss: 127.3834, Val Loss: 101.8099\n",
      "Early stopping at epoch 26516. Best validation loss: 69.0467\n",
      "Epoch 26516/30000, Train Loss: 126.7691, Val Loss: 79.9178\n",
      "Early stopping at epoch 26666. Best validation loss: 69.0467\n",
      "Epoch 26666/30000, Train Loss: 132.8057, Val Loss: 85.7793\n",
      "Early stopping at epoch 26816. Best validation loss: 69.0467\n",
      "Epoch 26816/30000, Train Loss: 122.2566, Val Loss: 93.6362\n",
      "Early stopping at epoch 26966. Best validation loss: 69.0467\n",
      "Epoch 26966/30000, Train Loss: 130.7780, Val Loss: 93.0651\n",
      "Epoch 27000/30000, Train Loss: 124.0801, Val Loss: 86.0985\n",
      "Early stopping at epoch 27116. Best validation loss: 69.0467\n",
      "Epoch 27116/30000, Train Loss: 124.6173, Val Loss: 92.1290\n",
      "Early stopping at epoch 27266. Best validation loss: 69.0467\n",
      "Epoch 27266/30000, Train Loss: 116.0626, Val Loss: 82.4917\n",
      "Early stopping at epoch 27416. Best validation loss: 69.0467\n",
      "Epoch 27416/30000, Train Loss: 128.7117, Val Loss: 107.4061\n",
      "Epoch 27500/30000, Train Loss: 122.4631, Val Loss: 89.8142\n",
      "Early stopping at epoch 27566. Best validation loss: 69.0467\n",
      "Epoch 27566/30000, Train Loss: 126.3667, Val Loss: 94.8484\n",
      "Early stopping at epoch 27716. Best validation loss: 69.0467\n",
      "Epoch 27716/30000, Train Loss: 120.5800, Val Loss: 84.5678\n",
      "Early stopping at epoch 27866. Best validation loss: 69.0467\n",
      "Epoch 27866/30000, Train Loss: 129.2812, Val Loss: 96.3493\n",
      "Epoch 28000/30000, Train Loss: 120.3942, Val Loss: 91.8160\n",
      "Early stopping at epoch 28016. Best validation loss: 69.0467\n",
      "Epoch 28016/30000, Train Loss: 123.6910, Val Loss: 96.4465\n",
      "Early stopping at epoch 28166. Best validation loss: 69.0467\n",
      "Epoch 28166/30000, Train Loss: 130.5956, Val Loss: 87.9491\n",
      "Early stopping at epoch 28316. Best validation loss: 69.0467\n",
      "Epoch 28316/30000, Train Loss: 133.7304, Val Loss: 90.5562\n",
      "Early stopping at epoch 28466. Best validation loss: 69.0467\n",
      "Epoch 28466/30000, Train Loss: 124.7689, Val Loss: 96.3822\n",
      "Epoch 28500/30000, Train Loss: 127.4880, Val Loss: 100.5203\n",
      "Early stopping at epoch 28616. Best validation loss: 69.0467\n",
      "Epoch 28616/30000, Train Loss: 118.9770, Val Loss: 88.3461\n",
      "Early stopping at epoch 28766. Best validation loss: 69.0467\n",
      "Epoch 28766/30000, Train Loss: 121.2889, Val Loss: 96.7464\n",
      "Early stopping at epoch 28916. Best validation loss: 69.0467\n",
      "Epoch 28916/30000, Train Loss: 120.8276, Val Loss: 87.7009\n",
      "Epoch 29000/30000, Train Loss: 115.5750, Val Loss: 97.3027\n",
      "Early stopping at epoch 29066. Best validation loss: 69.0467\n",
      "Epoch 29066/30000, Train Loss: 118.0525, Val Loss: 98.8068\n",
      "Early stopping at epoch 29216. Best validation loss: 69.0467\n",
      "Epoch 29216/30000, Train Loss: 124.8821, Val Loss: 92.5626\n",
      "Early stopping at epoch 29366. Best validation loss: 69.0467\n",
      "Epoch 29366/30000, Train Loss: 119.8501, Val Loss: 91.1019\n",
      "Epoch 29500/30000, Train Loss: 125.5193, Val Loss: 80.5312\n",
      "Early stopping at epoch 29516. Best validation loss: 69.0467\n",
      "Epoch 29516/30000, Train Loss: 118.7812, Val Loss: 102.0282\n",
      "Early stopping at epoch 29666. Best validation loss: 69.0467\n",
      "Epoch 29666/30000, Train Loss: 130.0538, Val Loss: 86.8992\n",
      "Early stopping at epoch 29816. Best validation loss: 69.0467\n",
      "Epoch 29816/30000, Train Loss: 123.9780, Val Loss: 102.4113\n",
      "Early stopping at epoch 29966. Best validation loss: 69.0467\n",
      "Epoch 29966/30000, Train Loss: 128.0049, Val Loss: 87.6379\n",
      "Epoch 30000/30000, Train Loss: 128.7733, Val Loss: 98.4896\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "patience = 150\n",
    "num_epochs = 30000\n",
    "train_losses = np.zeros(num_epochs)\n",
    "val_losses = np.zeros(num_epochs)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = load_and_preprocess_data(X, Y)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_network(model, optimizer, criterion, train_loader, val_loader, num_epochs, device, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE: 155.38978931669442\n"
     ]
    }
   ],
   "source": [
    "baseline_prediction = np.mean(Y)\n",
    "baseline_mse = np.mean((Y - baseline_prediction) ** 2)\n",
    "print(f'Baseline MSE: {baseline_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
